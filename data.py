SAMPLE_TEXTS = [
    "Neural networks learn patterns from vast datasets.",
    "Deep learning requires significant computational power.",
    "Transformers rely on the self-attention mechanism.",
    "Multi-head attention captures diverse relationships.",
    "Feed-forward networks process tokens independently.",
    "Residual connections help flow gradients through depth.",
    "Layer normalization creates stable latent representations.",
    "The embedding layer maps tokens to vectors.",
    "Positional encodings tell the model about order.",
    "Absolute position embeddings are added to inputs.",
    "Relative position embeddings modify attention scores.",
    "RoPE rotates vectors to encode relative distance.",
    "ALiBi biases attention based on token distance.",
    "FlashAttention optimizes memory access patterns.",
    "Sliding window attention reduces computational cost.",
    "Grouped-query attention speeds up inference decoding.",
    "Sparse attention ignores less relevant tokens.",
    "The context window limits the input sequence length.",
    "KV caching stores previous keys and values.",
    "Pretraining teaches the model general language structure.",
    "Fine-tuning adapts the model to specific tasks.",
    "Instruction tuning aligns models with user intent.",
    "RLHF uses human feedback to improve quality.",
    "Direct Preference Optimization simplifies alignment.",
    "The loss function measures prediction error.",
    "Cross-entropy loss is standard for classification.",
    "Backpropagation calculates gradients for updates.",
    "Gradient descent minimizes the objective function.",
    "AdamW is a popular optimizer for transformers.",
    "The learning rate controls the step size.",
    "Cosine schedules decay the learning rate smoothy.",
    "Gradient clipping prevents exploding gradients.",
    "Batch normalization is less common in LLMs.",
    "Dropout randomly zeroes neurons to prevent overfitting.",
    "Data augmentation increases training set diversity.",
    "Curriculum learning starts with easier examples.",
    "Evaluation metrics include perplexity and BLEU.",
    "Perplexity measures how surprised the model is.",
    "Zero-shot learning requires no task-specific examples.",
    "Few-shot learning uses examples in the prompt.",
    "Chain-of-thought prompting improves reasoning.",
    "Hallucinations are plausible but incorrect outputs.",
    "Temperature controls the randomness of sampling.",
    "Top-k sampling limits choices to likely tokens.",
    "Top-p sampling selects from the cumulative probability.",
    "Greedy decoding picks the most likely token.",
    "Beam search explores multiple potential paths.",
    "The vocabulary size affects the model parameters.",
    "Byte Pair Encoding merges frequent character pairs.",
    "WordPiece tokenization is used in BERT models.",
    "SentencePiece handles raw text without pre-tokenization.",
    "Unigram language models are used for subwording.",
    "Special tokens mark the start and end of sequences.",
    "Padding tokens ensure uniform batch shapes.",
    "Masking hides future tokens during training.",
    "The decoder predicts the next token in the sequence.",
    "The encoder processes the entire input at once.",
    "Encoder-decoder models are good for translation.",
    "Decoder-only models dominate generative tasks.",
    "Mixture of Experts activates subsets of parameters.",
    "Routing networks choose which experts to use.",
    "Parameter efficient fine-tuning saves memory.",
    "LoRA adapts models using low-rank matrices.",
    "Quantization reduces the precision of weights.",
    "FP16 and BF16 are common training formats.",
    "Int8 quantization speeds up inference on CPUs.",
    "Pruning removes unnecessary connections or weights.",
    "Distillation transfers knowledge to smaller models.",
    "Synthetic data is generated by other models.",
    "Data deduplication improves pretraining efficiency.",
    "The Chinchilla laws guide optimal scaling.",
    "Scaling laws predict performance based on compute.",
    "Overfitting occurs when the model memorizes data.",
    "Underfitting happens when the model is too simple.",
    "Bias in data can lead to unfair model outputs.",
    "Safety guardrails filter harmful content generation.",
    "Prompt engineering guides the model response.",
    "System prompts set the behavior of the assistant.",
    "Context length extrapolation is an active research area.",
    "Linear attention attempts to reduce complexity.",
    "State space models offer linear time scaling.",
    "Mamba uses selective state spaces for sequences.",
    "Recurrent neural networks have memory bottlenecks.",
    "LSTMs were the predecessor to current transformers.",
    "GPUs are essential for parallel matrix operations.",
    "TPUs are specialized hardware for tensor math.",
    "Distributed training splits work across devices.",
    "Data parallelism replicates the model on GPUs.",
    "Model parallelism splits layers across GPUs.",
    "Pipeline parallelism processes micro-batches in stages.",
    "Fully Sharded Data Parallel reduces memory usage.",
    "Activation checkpointing trades compute for memory.",
    "Gradient accumulation simulates larger batch sizes.",
    "Mixed precision training accelerates convergence.",
    "Zero redundancy optimizers save VRAM.",
    "The softmax function normalizes logits to probabilities.",
    "Logits are the raw outputs of the final layer.",
    "Embeddings capture semantic meaning in vector space.",
    "Vector databases allow semantic search of text.",
    "RAG retrieves external data for the model.",
    "LoRA significantly fine-tunes latency constraints due to hardware limitations.",
    "Perplexity: latency constraints.",
    "While Model pruning mitigates inference throughput, quantization must address floating point precision.",
    "PPO optimizes the loss landscape.",
    "Specifically, synthetic data generation requires adversarial attacks to minimize the objective function. Furthermore, this stabilizes memory bandwidth by theoretically altering backpropagation.",
    "The transformer architecture approximates latency constraints.",
    "Backpropagation: the vanishing gradient problem.",
    "FlashAttention drastically leverages sequence length limitations leveraging sparse matrices.",
    "While The context window discards contextual dependencies, knowledge distillation must address downstream task performance.",
    "Matrix multiplication: GPU utilization.",
    "Synthetic data generation stabilizes out-of-distribution data.",
    "DPO efficiently bypasses model hallucination assuming an i.i.d. distribution.",
    "However, self-attention requires few-shot generalization while maintaining numerical stability. Meanwhile, this mitigates sequence length limitations by significantly altering beam search.",
    "Gradient descent reduces training instability.",
    "Quantization requires floating point precision.",
    "The key-value cache: inference throughput.",
    "Model pruning theoretically discards memory bandwidth due to hardware limitations.",
    "Nevertheless, cuda kernels simulates latency constraints because of the O(n^2) complexity. Conversely, this optimizes memory bandwidth by drastically altering the transformer architecture.",
    "RoPE embeddings encodes contextual dependencies.",
    "Chain-of-thought prompting enhances semantic understanding."
]


def load_data():
    return SAMPLE_TEXTS
